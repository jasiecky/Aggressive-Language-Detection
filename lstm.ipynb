{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NUi1fTelLbIs",
    "outputId": "74a8f406-4689-49e0-d1e1-e82c4610df0b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%pip install contractions\n",
    "#%pip install word2number\n",
    "#%pip install unidecode\n",
    "#%pip install num2words\n",
    "#%pip install accelerate\n",
    "#%pip install scikeras\n",
    "#%pip uninstall sklearn\n",
    "#%pip install -U imbalanced-learn scikit-learn\n",
    "#%pip install tensorflow==2.12.0\n",
    "#%pip install inflect\n",
    "#!python -m nltk.downloader all\n",
    "#!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRSIRH7fV0wF",
    "outputId": "e65e39fe-b19d-4a64-f4bc-b21ec4b21722"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "import re\n",
    "import spacy\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import contractions\n",
    "from word2number import w2n\n",
    "import unidecode\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import num2words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import inflect\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12,5)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcaAjHrJNKCq"
   },
   "source": [
    "# 1. Cel\n",
    "Celem projektu jest stworzenie modelu analizy sentymentu, a dokładniej klasyfikatora binarnego wykrywającego agresję językową (obraźliwy język) we wpisach na portalu Twitter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wFVdOO0V0wI"
   },
   "source": [
    "# 2. Pozyskanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g14ac2HsG_OU"
   },
   "source": [
    "Dane pobrano ze strony Kaggle.com - [Hate Speech and Offensive Language Dataset](https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset). W pierwszej kolejności pobrano dane i usunięto niepotrzebne kolumny, zostawiając tylko class i tweet, czyli klasyfikację wpisu i jego treść. Domyślnie wpisy neutralne mają klasę 2, a agresywne 1, zaś klasa 0, czyli tzw. mowa nienawiści została całkowicie usunięta, wobec czego zmieniono klasę wpisów neutralnych na 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "PRrof9y3V0wJ",
    "outputId": "3e43ee2d-af0d-4413-a4e8-49b028a524d2"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"labeled_data.csv\")\n",
    "df.drop(columns = ['Unnamed: 0', \"count\", \"hate_speech\", \"offensive_language\", \"neither\"], inplace=True)\n",
    "df = df[df[\"class\"] != 0]\n",
    "df.replace({\"class\": {2: 0}}, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvIx1zYjMi_D"
   },
   "source": [
    "# 3. Wstępna ocena danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXlCJCqUHowc"
   },
   "source": [
    "Jak widać po poniższych wynikach operacja przebiegła pomyślnie. Ramka danych składa się teraz z dwóch kolumn, class przyjmuje wartość 0 lub 1, a w zbiorze danych nie ma wartości brakujących ani zduplikowanych wpisów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12LDaj_rOjXC",
    "outputId": "f56c68f8-55ba-41f0-ae5e-caeb3f4fee00"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q4Loibr46iz5",
    "outputId": "ad170b12-346b-4e91-f6eb-3b4e5113cc42"
   },
   "outputs": [],
   "source": [
    "# sprawdzamy czy na pewno mamy odpowiednią liczbę klas\n",
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "UcsI7-x15ocf",
    "outputId": "0df99a78-7b4f-462c-e672-6aee40e7b276"
   },
   "outputs": [],
   "source": [
    "# sprawdzamy czy istnieją zduplikowane tweety\n",
    "df[df.duplicated(['tweet'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GStjazye6ACP",
    "outputId": "df9b6cb1-fb37-469c-b8d1-9acafb8102eb"
   },
   "outputs": [],
   "source": [
    "# sprawdzamy czy istnieją brakujące wartości\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqGIuHaLH-S9"
   },
   "source": [
    "Poniżej wyświetlono przykładowe wpisy neutralne oraz agresywne. Można zauważyć, że wpisy agresywne charakteryzują się dużo większą liczbą wulgaryzmów oraz wyzwiskami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "yD25XODk63ll",
    "outputId": "dd7f81cd-ee95-4f6e-df5f-d24e53f93f30"
   },
   "outputs": [],
   "source": [
    "df[df['class'] == 0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "Bm0Ne9IH6-Pt",
    "outputId": "3ec58586-5ae3-49e8-8027-57082422b193"
   },
   "outputs": [],
   "source": [
    "df[df['class'] == 1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFYsUKKmIO96"
   },
   "source": [
    "Następnie sprawdzono unikalne znaki. Jak widać oprócz liter znajduje się również wiele znaków interpunkcyjnych oraz cyfr. W dodatku litery występują zarówno jako duże, jak i małe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZbqJM_TMFLf",
    "outputId": "a570ff30-496a-4d63-9a10-74863216b476"
   },
   "outputs": [],
   "source": [
    "print(\"Lista unikalnych znaków: \", set(df['tweet'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8_tED3KImiM"
   },
   "source": [
    "Liczba unikalnych słów w całym zbiorze danych to 51350. Jednakże wiele z nich to tak zwane \"stop words\" lub słowa charakterystyczne dla Twittera, np. rt, czyli retweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HovuZ_fMuAA",
    "outputId": "4b5b1594-eceb-41f0-8559-ba33c1db180b"
   },
   "outputs": [],
   "source": [
    "results = set()\n",
    "df['tweet'].str.lower().str.split().apply(results.update)\n",
    "print(\"Liczba unikalnych słów: \", len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O08zbtwZ6jq8",
    "outputId": "2c2a425c-ccb7-4a80-b236-f65f5b7b88fb"
   },
   "outputs": [],
   "source": [
    "# sprawdzamy najpopularniejsze słowa\n",
    "print(\"Najpopularniejsze słowa i ich liczność: \", Counter(\" \".join(df['tweet'].str.lower()).split()).most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyg_fHNFJCOr"
   },
   "source": [
    "### Liczność klas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-yySIRXJKcs"
   },
   "source": [
    "Sprawdzono liczność klas, jak widać na poniższym wykresie zbiór jest niezbalansowany i jest widoczna kilkukrotnie większa liczba wpisów agresywnych, niż neutralnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "GN1E6sdCI85M",
    "outputId": "330ce13f-3c2f-46a3-f846-36e12112e676"
   },
   "outputs": [],
   "source": [
    "# sprawdzamy liczność klas\n",
    "sns.countplot(x = \"class\", data = df)\n",
    "plt.title(\"Liczność klas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idetNJ0ZK-Bj"
   },
   "source": [
    "## Najczęściej występujące słowa\n",
    "\n",
    "Następnie sprawdzono najpopularniejsze słowa dla całego zbioru danych, agresywnych wpisów i wpisów neutralnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uK4KOi7CKMXs"
   },
   "outputs": [],
   "source": [
    "def plot_top_words(top_words, title):\n",
    "    words = [item[0] for item in top_words]\n",
    "    counts = [item[1] for item in top_words]\n",
    "    plt.bar(words, counts)\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "bFG3d9-6Jtz9",
    "outputId": "30ea8bf8-0690-4855-beed-1a9ba7b1ebb3"
   },
   "outputs": [],
   "source": [
    "top_words_for_whole_dataset = Counter(\" \".join(df['tweet'].str.lower()).split()).most_common(30)\n",
    "plot_top_words(top_words_for_whole_dataset, \"The most common 30 words in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "WfdQnmskKf2c",
    "outputId": "d84b7797-9ae0-4ce6-dc93-3953b27a1329"
   },
   "outputs": [],
   "source": [
    "top_words_for_agressive_tweets = Counter(\" \".join(df[df['class']==1][\"tweet\"].str.lower()).split()).most_common(30)\n",
    "plot_top_words(top_words_for_agressive_tweets, \"The most common 30 words in agressive tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "CkRXjsi1K0Uz",
    "outputId": "376689da-59a2-4af8-9847-44a8e01cba6a"
   },
   "outputs": [],
   "source": [
    "top_words_for_neutral_tweets = Counter(\" \".join(df[df['class']==0][\"tweet\"].str.lower()).split()).most_common(30)\n",
    "plot_top_words(top_words_for_neutral_tweets, \"The most common 30 words in neutral tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lg1Xo-YGV0wK"
   },
   "source": [
    "W celu lepszego zwizualizowania najczęściej występujących słów utworzono \"world cloud\" dla całego zbioru danych, wpisów agresywnych i neutralnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGdy64PiMUFO"
   },
   "outputs": [],
   "source": [
    "def plot_word_cloud(words, title):\n",
    "    wordCloud = WordCloud(width=400, height=300, random_state=100, max_font_size=100).generate(words)\n",
    "    plt.figure(figsize=(11, 9))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(wordCloud, interpolation=\"bilinear\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def join_tweets_by_class(data, class_label=None):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        if class_label is not None:\n",
    "            tweets = ' '.join([tweet for tweet in data[data['class'] == class_label]['tweet']])\n",
    "        else:\n",
    "            tweets = ' '.join([tweet for tweet in data['tweet']])\n",
    "\n",
    "    if isinstance(data, pd.Series):\n",
    "        tweets = ' '.join([tweet for tweet in data])\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "yCey3Ec_V0wK",
    "outputId": "0409da39-4062-46fe-9bef-2726cbb59367"
   },
   "outputs": [],
   "source": [
    "all_words = join_tweets_by_class(df)\n",
    "plot_word_cloud(all_words, \"Word cloud for whole dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "gpUnYBZRV0wL",
    "outputId": "08da01a8-6e07-4a25-d355-0d34ac50c77d"
   },
   "outputs": [],
   "source": [
    "agressive_tweets_words = join_tweets_by_class(df, 1)\n",
    "plot_word_cloud(agressive_tweets_words, \"Word cloud for agressive tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "c_p3OaKTV0wL",
    "outputId": "5cf0279f-0c9e-40f4-e33a-225e2bfc3faf"
   },
   "outputs": [],
   "source": [
    "neutral_tweets_words = join_tweets_by_class(df, 0)\n",
    "plot_word_cloud(neutral_tweets_words, \"Word cloud for neutral tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb6PMkbKTb-t"
   },
   "source": [
    "W związku z przewagą wpisów agresywnych w zbiorze danych najczęstsze słowa w całym zbiorze są zdominowane przez słowa charakterystyczne dla wpisów agresywnych. Uwagę zwraca również obecność dużej liczby tagów Twitter'a - \"rt\" oraz innych słów nie mających wpływu na końcowe znaczenie zdania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bxj6t-JngxMA"
   },
   "source": [
    "## Rozkład długości wpisów pod względem liczby znaków\n",
    "Obliczono statysyki opisowe oraz utworzono histogramy i wykresy pudełkowe w celu prześledzenia rozkładu długości wpisów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taBF4QUsSpZ6",
    "outputId": "44203a83-5ef6-4c70-f70a-7eeb4e4d538b"
   },
   "outputs": [],
   "source": [
    "df['tweet_length'] = df['tweet'].apply(len)\n",
    "print(\"Statystyki opisowe dla długości wpisów w całym zbiorze danych: \\n\", df[\"tweet_length\"].describe(), \"\\n\")\n",
    "print(\"Statystyki opisowe dla długości wpisów agresywnych: \\n\", df[df['class'] == 1][\"tweet_length\"].describe(), \"\\n\")\n",
    "print(\"Statystyki opisowe dla długości wpisów neutralnych: \\n\", df[df['class'] == 0][\"tweet_length\"].describe(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "7-Hcdwts-ILf",
    "outputId": "097b7da9-1f65-49c1-c029-d1ef31c62367"
   },
   "outputs": [],
   "source": [
    "# sprawdzamy rozkład długości wpisów pod względem liczby znaków\n",
    "sns.displot(df['tweet_length'])\n",
    "plt.title(\"Distribution of tweet lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "YAg4yA-YqnFm",
    "outputId": "6d93318d-567e-4378-990f-b9cb2cb82a54"
   },
   "outputs": [],
   "source": [
    "# sprawdzamy rozkład długości wpisów w poszczególnych klasach\n",
    "graph = sns.FacetGrid(data=df, col='class')\n",
    "graph.map(plt.hist, 'tweet_length', bins=40)\n",
    "graph.set_titles(\"Class - {col_name}\")\n",
    "plt.suptitle(\"Distribution of tweet lenghts by class\", y=1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "EH1vymVvxhzL",
    "outputId": "6143c958-19bc-4e66-99d9-4842e379ce5e"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(y='tweet_length', x='class', data=df)\n",
    "plt.title(\"Tweet length by class\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Tweet Length\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TAiiD7yRfGN"
   },
   "source": [
    "Rozkłady długości wpisów w całym zbiorze danych oraz w poszczególnych klasach są zbliżone. Uwagę zwraca większa liczba wartości odstających w przypadku wpisów agresywnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HxMQPFSYEfoc",
    "outputId": "0d79bef1-b2da-4e4c-ffb4-8734fbd022c3"
   },
   "outputs": [],
   "source": [
    "df['number_of_words'] = df['tweet'].str.split().apply(len)\n",
    "print(\"Statystyki opisowe dla długości wpisów pod względem liczby słów w całym zbiorze danych: \\n\", df[\"number_of_words\"].describe(), \"\\n\")\n",
    "print(\"Statystyki opisowe dla długości wpisów agresywnych pod względem liczby słów: \\n\", df[df['class'] == 1][\"number_of_words\"].describe(), \"\\n\")\n",
    "print(\"Statystyki opisowe dla długości wpisów neutralnych pod względem liczby słów: \\n\", df[df['class'] == 0][\"number_of_words\"].describe(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctHe0okMe7BP"
   },
   "source": [
    "## Rozkład długości wpisów pod względem liczby słów\n",
    "Obliczono statysyki opisowe oraz utworzono histogramy i wykresy pudełkowe w celu prześledzenia rozkładu liczby słów we wpisach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "hMdr7n1UFNeS",
    "outputId": "45659410-f018-4cb0-f3e0-a127b182b196"
   },
   "outputs": [],
   "source": [
    "sns.displot(df['number_of_words'])\n",
    "plt.title(\"Distribution of number of words in tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "Dzt1tRY2FV3X",
    "outputId": "6ea9104a-cc86-434a-964c-5da11044eb63"
   },
   "outputs": [],
   "source": [
    "graph = sns.FacetGrid(data=df, col='class')\n",
    "graph.map(plt.hist, 'number_of_words', bins=30)\n",
    "graph.set_titles(\"Class - {col_name}\")\n",
    "plt.suptitle(\"Distribution of number of words in tweet by class\", y=1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "f2lyRgcLFcNm",
    "outputId": "3ad688fc-7607-475b-db1f-40d8fa3b14ab"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(y='number_of_words', x='class', data=df)\n",
    "plt.title(\"Number of words in tweet by class\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of words in tweet\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Si0DB4GjF47w"
   },
   "source": [
    "Również w przypadku liczby słów we wpisach rozkłady w całym zbiorze danych oraz w poszczególnych klasach są zbliżone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZdX4RnuV0wM"
   },
   "source": [
    "# 4. Przygotowanie danych do modelowania\n",
    "\n",
    "Po wstępnej ocenie danych rozpoczęto ich przetwarzanie. W pierwszej kolejności podzielono ramkę danych na X i y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQUaLz7DlVwh"
   },
   "source": [
    "### Podział na X i y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVkV12IiV0wM"
   },
   "outputs": [],
   "source": [
    "X = df['tweet'].copy()\n",
    "y = df['class'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9cZ0a2flbnR"
   },
   "source": [
    "\n",
    "### Przygotowanie funkcji oczyszczających dane\n",
    "\n",
    "Po podziale zbioru przygotowano funkcje mające na celu przeprowadzenie poszczególnych elementów normalizacji oraz lematyzacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gktjJzwi-eeu"
   },
   "outputs": [],
   "source": [
    "stop_words_extended = [\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]\n",
    "\n",
    "def lower(tweet: str) -> str:\n",
    "  return tweet.lower()\n",
    "\n",
    "def clean_tweet(tweet: str) -> str:\n",
    "    tweet = re.sub(r'http\\S+', ' ', tweet) # usuwanie url\n",
    "    tweet = re.sub(r'<.*?>',' ', tweet) # usuwanie tagów html\n",
    "    tweet = re.sub(r'#\\w+',' ', tweet) # usuwanie hasztagów\n",
    "    tweet = re.sub(r'@\\w+',' ', tweet) # usuwanie oznaczeń\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def remove_punctuation(tweet: list) -> list:\n",
    "    sentence = []\n",
    "    for word in tweet:\n",
    "      word = re.sub(r'[^\\w\\s]','', word) # usuwanie wszystkiego co nie jest literami oraz interpunkcją\n",
    "      word = word.replace('-', '')\n",
    "      word = word.replace('_', '')\n",
    "      word = word.replace(',', '')\n",
    "      word = re.sub('[^A-Za-z0-9]+', '', word) # usuwanie wszystkiego co nie jest literami oraz cyframi\n",
    "      sentence.append(word) if word != '' else None\n",
    "    return sentence\n",
    "\n",
    "def remove_twitter_tags(tweet: list) -> list:\n",
    "    twitter_tags = (\"ff\", \"rt\")\n",
    "    tweet = [word for word in tweet if word not in twitter_tags]\n",
    "    return tweet\n",
    "\n",
    "def remove_stop_words(tweet: list, stop_words_extended: list=stop_words_extended) -> list:\n",
    "    stop_words_set = set(stopwords.words('english'))\n",
    "    stop_words_set = stop_words_set.union(set(stop_words_extended))\n",
    "\n",
    "    tweet = [word for word in tweet if word not in stop_words_set]\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def stemming(tweet: list) -> list:\n",
    "    stemmer = PorterStemmer()\n",
    "    tweet = [stemmer.stem(word) for word in tweet]\n",
    "    return tweet\n",
    "\n",
    "def tokenize_tweet(tweet: str) -> list:\n",
    "  tokenized_tweet = nltk.word_tokenize(tweet)\n",
    "  return tokenized_tweet\n",
    "\n",
    "def remove_extra_whitespaces(tweet: str) -> str:\n",
    "  tweet = re.sub(' +',' ', tweet) # jedna lub więcej spacji\n",
    "  return tweet\n",
    "\n",
    "def expand_contractions(tweet: str) -> str:\n",
    "  tweet = contractions.fix(tweet)\n",
    "  return tweet\n",
    "\n",
    "def unidecode_characters(tweet: str) -> str:\n",
    "  tweet = unidecode.unidecode(tweet)\n",
    "  return tweet\n",
    "\n",
    "def join_list_to_sentence(list_of_words: list) -> str:\n",
    "  sentence = ' '.join(list_of_words)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence\n",
    "\n",
    "def transform_numbers_to_words(word_list: list) -> list:\n",
    "    engine = inflect.engine()\n",
    "    transformed_list = []\n",
    "\n",
    "    for word in word_list:\n",
    "        if word.isdigit():\n",
    "            transformed_word = engine.number_to_words(word)\n",
    "            transformed_list.append(transformed_word)\n",
    "        else:\n",
    "            transformed_list.append(word)\n",
    "\n",
    "    return transformed_list\n",
    "\n",
    "def lemmatize_words(word_list: list) -> list:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "\n",
    "    for word in word_list:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        lemmatized_words.append(lemmatized_word)\n",
    "\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtjVU_-9lfKh"
   },
   "source": [
    "### Normalizacja\n",
    "Po przygotowaniu odpowiednich funkcji przystąpiono do normalizacji danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F3ycxzd0WY4w",
    "outputId": "69e6fd97-0880-4b7c-bb46-d36ad71f990d"
   },
   "outputs": [],
   "source": [
    "X.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xd_Gx9-9W6lW"
   },
   "source": [
    "Normalizacje rozpoczęto od ujednolicenia wielkości liter poprzez zmniejszenie ich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qo7HLl-FHtl7",
    "outputId": "80de7bd8-4ea2-459e-d1cc-70387b20b705"
   },
   "outputs": [],
   "source": [
    "# zmniejszanie liter\n",
    "X_lowed = X.apply(lower)\n",
    "X_lowed.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZqoHRYdXHqU"
   },
   "source": [
    "Następnie usunięto zbędne elementy, jak na przykład tagi HTMl albo elementy adresów URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzywEO39bsKV",
    "outputId": "63a4800b-492c-481c-a589-cc6ed5eab8fb"
   },
   "outputs": [],
   "source": [
    "# usuwanie niepotrzebnych elementów, np.tagi HTML\n",
    "X_cleaned = X_lowed.apply(clean_tweet)\n",
    "X_cleaned.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD-8KHaHXiT1"
   },
   "source": [
    "Po usunięciu zbędnych elementów przeprowadzono unidecoding, czyli zmianę formatu unicode na format ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfAYvIXUb9oQ",
    "outputId": "678d4166-b754-4d63-98a2-e667cb7c95d4"
   },
   "outputs": [],
   "source": [
    "# unidecoding\n",
    "X_without_accented_characters = X_cleaned.apply(unidecode_characters)\n",
    "X_without_accented_characters.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LK57A6USXkR1"
   },
   "source": [
    "Następnie usunięto ze zbioru nadmierne odstępy między słowami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQX9xhaWcQTu",
    "outputId": "36ea51be-46ea-4f9e-895e-3d31fdc97bfd"
   },
   "outputs": [],
   "source": [
    "# usuwanie nadmiernych spacji\n",
    "X_without_extra_whitespaces = X_without_accented_characters.apply(remove_extra_whitespaces)\n",
    "X_without_extra_whitespaces.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFhIakRMXsou"
   },
   "source": [
    "Kolejnym krokiem była zamiana skróconych wersji słów do ich pełnych form, przykładowo I've -> I have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FokFMdn6Rf4v",
    "outputId": "bc3177e1-c8c6-4479-9102-ec92bc4c8102"
   },
   "outputs": [],
   "source": [
    "# expand contractions\n",
    "X_with_expanded_contractions = X_without_extra_whitespaces.apply(expand_contractions)\n",
    "X_with_expanded_contractions.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmbHinPAYIIt"
   },
   "source": [
    "Następnie przeprowadzono tokenizację."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYSYxE6HGgWB",
    "outputId": "28a32769-506e-45f7-f285-bec18482f511"
   },
   "outputs": [],
   "source": [
    "# tokenizacja\n",
    "X_tokenized = X_with_expanded_contractions.apply(tokenize_tweet)\n",
    "X_tokenized.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qvq-yqQYJ0VR"
   },
   "source": [
    "Następnie zamieniono wartości liczbowe do odpowiadających im słowom.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybJF4qDyJy9T",
    "outputId": "ca0bcf79-26db-4e74-b882-3e2732a41598"
   },
   "outputs": [],
   "source": [
    "# zamiana liczb na słowa\n",
    "X_without_numbers = X_tokenized.apply(transform_numbers_to_words)\n",
    "X_without_numbers.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtACa4pPYJ9N"
   },
   "source": [
    "W kolejnym kroku usunięto znaki interpunkcyjne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3FWUsOoYuvT",
    "outputId": "259adb9e-f172-4b34-9f6f-d6cdfb6011c0"
   },
   "outputs": [],
   "source": [
    "# usuwanie znaków interpunkcyjnych\n",
    "X_without_punctuation = X_without_numbers.apply(remove_punctuation)\n",
    "X_without_punctuation.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpopHVy-s0l-"
   },
   "source": [
    "Następnie usunięto słowa ff i rt, czyli tagi na Twitter'ze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWvyUWs-ssRt",
    "outputId": "308cb107-1804-46a9-cd57-140a3baf9fac"
   },
   "outputs": [],
   "source": [
    "# usuwanie tagów Twitter\n",
    "X_without_twitter_tags = X_without_punctuation.apply(remove_twitter_tags)\n",
    "X_without_twitter_tags.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVglftX7YPEN"
   },
   "source": [
    "Po usunięciu tagów usunięto tzw. \"stop words\", czyli słowa mające mały wpływ na końcowe znaczenie zdania, na przykład: a, an, the."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YP2fnVAhILy_",
    "outputId": "6d299c67-2825-4da4-848f-e4adb93af74b"
   },
   "outputs": [],
   "source": [
    "# usuwanie stop words\n",
    "X_without_stop_words = X_without_twitter_tags.apply(remove_stop_words)\n",
    "X_without_stop_words.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P7ojtrtly_p"
   },
   "source": [
    "### Stemming\n",
    "\n",
    "Na znormalizowanym zbiorze danych można przeprowadzić stemming lub lematyzację, czyli redukcję słów do ich bazowej formy. W eksperymencie zdecydowano się na wykorzystanie lematyzacji, która służy do tego samego zadania co stemming, ale zachowuje znaczenie słów, np. nie ujednolica training i train do tego samego znaczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXTYK3_jFTUp"
   },
   "outputs": [],
   "source": [
    "# stemming\n",
    "# X_stemmed = X_without_stop_words.apply(stemming)\n",
    "# X_stemmed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEpgbKa__m8-"
   },
   "source": [
    "### Lematyzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKyVlJPG_XWb",
    "outputId": "6614d6f5-8321-4f37-88df-fe712faf19fa"
   },
   "outputs": [],
   "source": [
    "# lematyzacja\n",
    "X_lemmatized = X_without_stop_words.apply(lemmatize_words)\n",
    "X_lemmatized.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sW4TrDuidQ6H",
    "outputId": "e45e65ea-fa4d-4cb3-cdc4-b9d8ce542dbf"
   },
   "outputs": [],
   "source": [
    "X_preprocessed = X_lemmatized.apply(join_list_to_sentence)\n",
    "X_preprocessed.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGkZ5x2uZav8"
   },
   "source": [
    "## Sprawdzenie skuteczności przeprowadzonych operacji\n",
    "\n",
    "Po przeprowadzeniu normalizacji oraz lematyzacji ponownie obliczono statystyki oraz wygenerowano wykresy użyte w poprzedniej fazie eksperymentu w celu upewnienia się, że zastosowane zabiegi były skuteczne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lkTi3b0AMQia",
    "outputId": "67089dd9-7669-4147-93de-5cb52ce9a2f8"
   },
   "outputs": [],
   "source": [
    "# sprawdzamy unikalne symbole po oczyszczeniu danych\n",
    "print(\"Lista unikalnych znaków po przetworzeniu danych: \", set(X_preprocessed.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfrg4e6sMl6H",
    "outputId": "6c28f0ad-72b4-45ac-ed40-da5c42a4a707"
   },
   "outputs": [],
   "source": [
    "# sprawdzamy liczbę unikalnych słów po przetworzeniu danych\n",
    "results = set()\n",
    "X_preprocessed.str.lower().str.split().apply(results.update)\n",
    "print(\"Liczba unikalnych słów po przetworzeniu danych: \", len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ioravnP4cgQ",
    "outputId": "82de8d5f-8a12-4200-a065-5649f7d1ba03"
   },
   "outputs": [],
   "source": [
    "# sprawdzamy najpopularniejsze słowa\n",
    "Counter(\" \".join(X_preprocessed).split()).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "BeR6YsesbCOx",
    "outputId": "e895efe8-9e56-4ad3-b33d-1946c822d143"
   },
   "outputs": [],
   "source": [
    "all_words_preprocessed = join_tweets_by_class(X_preprocessed)\n",
    "plot_word_cloud(all_words_preprocessed, \"Word cloud for whole dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "rNalOkVKbfSE",
    "outputId": "5d09de30-32b0-4945-e5d6-e4210751c5b0"
   },
   "outputs": [],
   "source": [
    "top_words_for_whole_dataset_preprocessed = Counter(\" \".join(X_preprocessed.str.lower()).split()).most_common(30)\n",
    "plot_top_words(top_words_for_whole_dataset_preprocessed, \"The most common 30 words in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSqr0QShqaKo"
   },
   "outputs": [],
   "source": [
    "X_preprocessed_copy = X_preprocessed.copy()\n",
    "X_preprocessed_copy = pd.DataFrame(X_preprocessed_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sq1xcOt5kT84",
    "outputId": "c111d6bb-2529-4058-dd25-c5a4431d03a2"
   },
   "outputs": [],
   "source": [
    "X_preprocessed_copy['tweet_length'] = X_preprocessed_copy[\"tweet\"].apply(len)\n",
    "\n",
    "sns.boxplot(y='tweet_length', data=X_preprocessed_copy)\n",
    "plt.title(\"Tweet length\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Tweet Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "ihhv2yD5kqOJ",
    "outputId": "458ae61d-e132-4639-fa4b-32c7188cfd2c"
   },
   "outputs": [],
   "source": [
    "X_preprocessed_copy['number_of_words'] = X_preprocessed_copy[\"tweet\"].str.split().apply(len)\n",
    "sns.displot(X_preprocessed_copy['number_of_words'])\n",
    "plt.title(\"Distribution of number of words in tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXGhDGps-Ztu"
   },
   "source": [
    "## Rozkład klas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "c6ZfpzuE-VYs",
    "outputId": "0d603276-e753-422d-e5f5-34eff1ad583a"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=y)\n",
    "plt.title(\"Liczność klas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUCc0QrLV0wN"
   },
   "source": [
    "## Podział na zbiór treningowy, walidacyjny i testowy\n",
    "\n",
    "Następnie dokonano podziału na zbiór treningowy, walidacyjny i testowy w proporcji 70:15:15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNX2JdrsTkjR"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.3, random_state=101)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vq3d1e9CHuM",
    "outputId": "8c112bc2-ddf3-4721-9920-43dfd2c067eb"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HXXdmlf-E1H"
   },
   "source": [
    "## Oversampling\n",
    "\n",
    "Jak zauważono na początku klasy są niezbalansowane, więc zdecydowano się na oversampling wpisów neutralnych dla zbioru treningowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6LlcSQ3GT7x"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "y_train_reshaped = np.array(y_train).reshape(-1, 1)\n",
    "X_train_reshaped = np.array(X_train.values).reshape(-1, 1)\n",
    "rus = RandomOverSampler(random_state=42)\n",
    "X_train, y_train = rus.fit_resample(X_train_reshaped, y_train_reshaped)\n",
    "X_train = pd.Series(X_train.flatten())\n",
    "y_train = pd.Series(y_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "rOMPEV1L998f",
    "outputId": "b72a7446-b58b-4a13-cfdd-0d779d548a9c"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=y_train)\n",
    "plt.title(\"Liczność klas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFlRIf9v-lF1"
   },
   "outputs": [],
   "source": [
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_val = tf.convert_to_tensor(y_val)\n",
    "y_test = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGzsGy3Xpawr"
   },
   "source": [
    "## Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ft5TtGEqotFq"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=128, padding='post', truncating='post')\n",
    "X_val = pad_sequences(X_val, maxlen=128, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test, maxlen=128, padding='post', truncating='post')\n",
    "\n",
    "vocabulary_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_train_data = (X_train, y_train)\n",
    "llm_val_data = (X_val, y_val)\n",
    "llm_test_data = (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koe82VgdV0wO"
   },
   "source": [
    "# 5. Modelowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(model_history):\n",
    "    epochs = range(1, len(model_history.history['loss']) + 1)\n",
    "    loss = model_history.history['loss']\n",
    "    validation_loss = model_history.history['val_loss']\n",
    "    accuracy = model_history.history['accuracy']\n",
    "    validation_accuracy = model_history.history['val_accuracy']\n",
    "\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.plot(epochs, validation_loss, 'r', label='Validation Loss',)\n",
    "    plt.plot(epochs, loss, 'b', label='Training Loss',)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xticks(epochs)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy_history(model_history):\n",
    "    epochs = range(1, len(model_history.history['accuracy']) + 1)\n",
    "    accuracy = model_history.history['accuracy']\n",
    "    validation_accuracy = model_history.history['val_accuracy']\n",
    "\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.plot(epochs, validation_accuracy, 'r', label='Validation Accuracy')\n",
    "    plt.plot(epochs, accuracy, 'b', label='Training Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.xticks(epochs)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import shutil\n",
    "import os\n",
    " \n",
    "def copy_and_replace(source_path, destination_path):\n",
    "    if os.path.exists(destination_path):\n",
    "        os.remove(destination_path)\n",
    "    shutil.copy2(source_path, destination_path)\n",
    " \n",
    "\n",
    "def grid_search(X_train, y_train, X_val, y_val, vocabulary_size, model_name, function_to_create_a_model, epochs_list = [5, 10, 15, 20, 25, 30], batch_size_list = [16, 32, 64, 128],):\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "\n",
    "    try:\n",
    "        os.makedirs(model_name)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    for epochs in epochs_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            \n",
    "            print(f\"Training with epochs={epochs}, batch_size={batch_size}\")\n",
    "            \n",
    "            cv_scores = []\n",
    "            model = function_to_create_a_model(vocabulary_size)\n",
    "\n",
    "            # callbacks\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            csv_logger = CSVLogger(f'{model_name}/history.log', separator=',', append=False)\n",
    "            \n",
    "            # Train the model\n",
    "            model_history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                        epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, csv_logger], verbose=1)\n",
    "\n",
    "            # Evaluate the model\n",
    "            y_pred = model.predict(X_val).ravel()\n",
    "            y_pred = tf.cast(tf.squeeze(tf.round(y_pred)), dtype=tf.int32)\n",
    "            accuracy = accuracy_score(y_val, y_pred)\n",
    "            cv_scores.append(accuracy)\n",
    "            avg_score = np.mean(cv_scores)\n",
    "\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = {'epochs': epochs, 'batch_size': batch_size}\n",
    "\n",
    "                with open(f'{model_name}/the_best_parameters.json', \"w\") as f:\n",
    "                    json.dump(best_params,f)\n",
    "\n",
    "                model.save(f'{model_name}/the_best_model.keras', overwrite=True)\n",
    "                copy_and_replace(f'{model_name}/history.log', f'{model_name}/the_best_model_history.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njShGwiqyx_w"
   },
   "source": [
    "## Sieć neuronowa LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(vocabulary_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocabulary_size, output_dim=128))\n",
    "    model.add(LSTM(units=128, activation='tanh', return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(units=64, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search(X_train, y_train, X_val, y_val, vocabulary_size, \"lstm\", create_lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQMagolfOXTE"
   },
   "outputs": [],
   "source": [
    "plot_training_history(lstm_model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nx6U9GVuPDQd"
   },
   "outputs": [],
   "source": [
    "plot_accuracy_history(lstm_model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aokdczOUaoJX"
   },
   "source": [
    "# 6. Ewaluacja\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aAMmgpTbDAz"
   },
   "outputs": [],
   "source": [
    "def prepare_confusion_matrix(preds, title):\n",
    "    predictions_result = tf.cast(tf.squeeze(tf.round(preds)), dtype=tf.int32)\n",
    "    conf = confusion_matrix(y_test, predictions_result)\n",
    "    cm = pd.DataFrame(\n",
    "        conf, index=[f'Wpis w rzeczywistości {i}' for i in ['neutralny', 'agresywny']],\n",
    "        columns=[f'Wpis przewidziany jako {i}' for i in ['neutralny', 'agresywny']]\n",
    "    )\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "    plt.ylabel('Rzeczywista wartość')\n",
    "    plt.xlabel('Przewidziana wartość')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def prepare_class_report(preds, message):\n",
    "  print(message)\n",
    "  predictions_result = tf.cast(tf.squeeze(tf.round(preds)), dtype=tf.int32)\n",
    "  print(classification_report(y_test, predictions_result, target_names=['Neutralny','Agresywny']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7n-5zL6ayKv"
   },
   "source": [
    "## Sieć LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfoSnnKma1gG"
   },
   "outputs": [],
   "source": [
    "lstm_model = tf.keras.models.load_model(\"lstm/the_best_model.keras\")\n",
    "lstm_model_y_pred = lstm_model.predict(X_test)\n",
    "lstm_model_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EGl0RLlc_Ls"
   },
   "outputs": [],
   "source": [
    "prepare_class_report(lstm_model_y_pred, \"Raport klasyfikacji dla sieci LSTM:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0kFWJ9udAoJ"
   },
   "outputs": [],
   "source": [
    "prepare_confusion_matrix(lstm_model_y_pred, 'Macierz omyłek dla sieci LSTM')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 723100,
     "sourceId": 1257215,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
